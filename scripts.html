<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8" />
		<title>Programmation et Projet encadré</title>
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css" />
		<meta name="viewport" content="width=device-width, initial-scale=1">
	</head>
	<body class="has-navbar-fixed-top">
		<!--NAVBAR-->
		<nav class="navbar is-light is-fixed-top">
			<div class="navbar-menu">
				<div class="navbar-start">
					<div class="navbar-item has-dropdown is-hoverable">
						<a class="navbar-item" href="index.html#introduction">Introduction</a>
					</div>
					<div class="navbar-item has-dropdown is-hoverable">
						<a class="navbar-item" href="index.html#analyse">Analyse</a>
					</div>
					<div class="navbar-item has-dropdown is-hoverable">
						<a class="navbar-item">Scripts</a>
					</div>
					<div class="navbar-item has-dropdown is-hoverable">
						<a class="navbar-item">Visualisations</a>
						<div class="navbar-dropdown">
							<a class="navbar-item" href="visualization/idees_societe.html">
							  Idées Société
							</a>
							<a class="navbar-item" href="visualization/politique_economie.html">
							  Politique Économie
							</a>
							<a class="navbar-item" href="visualization/planete_voyage.html">
							  Planète Voyage
							</a>
						  </div>
					</div>
				</div>
				<div class="navbar-end">
					<div class="navbar-item has-dropdown is-hoverable">
						<a class="navbar-item" href="index.html#aPropos">À propos</a>
					</div>
					<div class="navbar-item">
						<a href="https://gitlab.com/cbuontal/ppe2-totally_spies"><img src="images/gitlab_logo.png" alt="Gitlab"></a>
					</div>
				</div>
			</div>
		</nav>
		<!--HEADER-->
		<header style="background-color: #06213e;">
			<h1 class="title has-text-centered" style="color: #ffffff; font-size: 40px; letter-spacing: -0.02em; font-variant-caps: small-caps;">Programmation et Projet Encadré 2 :<br/>Topic Modeling</h1>
		</header>
		<!--CONTENU-->
		<div class="has-text-justified", style="background-color: white;">
			<!--run_lda-->
			<h1 class="title has-text-centered" style="color: #a92927; font-size: 40px; letter-spacing: -0.02em; font-variant-caps: small-caps;">run_lda.py</h1>
			<div class="columns">
				<div class="column is-one-fifth"></div>
				<div class="column" style="border-left: 5px solid #a92927; overflow: auto; max-height: 1000px;">
					<p class="block">
						Ce script nous permet de définir les topics à l'aide de la bibliothèque Gensim et de produire les pages .html nous permettant l'analyse de ceux-ci à l'aide de la bibliothèque pyLDAvis.
					</p>
					<pre>
						<code>
r"""
LDA Model
=========

Introduces Gensim's LDA model and demonstrates its use on the NIPS corpus.

"""

#import nltk
#nltk.download('wordnet')

import argparse, sys
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

import io
import os.path
import re
import tarfile

import smart_open

def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):
    with smart_open.open(url, "rb") as file:
        with tarfile.open(fileobj=file) as tar:
            for member in tar.getmembers():
                if member.isfile() and re.search(r'nipstxt/nips\d+/\d+\.txt', member.name):
                    member_bytes = tar.extractfile(member).read()
                    yield member_bytes.decode('utf-8', errors='replace')

#docs = list(extract_documents())

# Lemmatize the documents.
#from nltk.stem.wordnet import WordNetLemmatizer

#lemmatizer = WordNetLemmatizer()
#docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]
print("DOCS GENSIM : ...")
#print(docs)

# docs est une liste de lemmes
from xml.etree import ElementTree as et

def conversion_xml(xml_file:str, lem_forme:str, cat_gram:list):
    """
        Prend en entrée :
            - un nom de fichier xml,
            - une chaîne pouvant prendre deux valeurs : 'lemme' ou 'forme',
            - une liste de catégories grammaticales à conserver dans l'analyse.
                si la liste est vide, on prend tous les tokens

        Produit une sortie la liste des lemmes (str) du document par article :

        [
            [art1_lemma1, art1_lemma2, …],
            [art2_lemma1, art2_lemma2, …],
            ...
        ]
    """
    output = []
    with open(xml_file, 'r') as f:
        contenu = f.read()
    # parser XML
    tree = et.parse(xml_file)
    root = tree.getroot()

    if len(cat_gram) == 0:
        # pour chaque article
        for article in root.iter('article'):
            tokens = []
            for token in article.iter('token'):
                lemme = token.get(lem_forme)
                if lemme is not None:
                    tokens.append(lemme)
            output.append(tokens)
    else:
        for article in root.iter('article'):
            tokens = []
            for token in article.iter('token'):
                lemme = token.get(lem_forme)
                # on filtre les catégories grammaticales
                categorie = token.get('pos')
                if lemme is not None and categorie in cat_gram:
                    tokens.append(lemme)
            output.append(tokens)
    return output

import pickle

def conversion_pkl(pkl_file:str, lem_forme:str, cat_gram:list):
    """
        Принимает на вход:
            - имя файла pkl,
            - строку, которая может принимать два значения: 'lemme' или 'forme',
            - список грамматических категорий для анализа.
              Если список пуст, выбираются все токены.

        Возвращает список лемм (str) для каждой статьи в документе:

        [
            [art1_lemma1, art1_lemma2, …],
            [art2_lemma1, art2_lemma2, …],
            ...
        ]
    """
    output = []
    with open(pkl_file, 'rb') as f:
        contenu = pickle.load(f)

    if len(cat_gram) == 0:
        # для каждой статьи
        for article in contenu.articles:
            tokens = []
            for token in article.analyse:
                if lem_forme == 'lemme':
                    lemme = token.lemme
                else:
                    lemme = token.forme
                if lemme is not None:
                    tokens.append(lemme)
            output.append(tokens)
    else:
        for article in contenu.articles:
            tokens = []
            for token in article.analyse:
                if lem_forme == 'lemme':
                    lemme = token.lemme
                else:
                    lemme = token.forme
                # фильтруем грамматические категории
                categorie = token.pos
                if lemme is not None and categorie in cat_gram:
                    tokens.append(lemme)
            output.append(tokens)
    return output

import json

def conversion_json(json_file: str, lem_forme: str, cat_gram: list):
    """
        Принимает на вход:
            - имя файла JSON,
            - строку, которая может принимать два значения: 'lemme' или 'forme',
            - список грамматических категорий для анализа.
              Если список пуст, выбираются все токены.

        Возвращает список лемм (str) для каждой статьи в документе:

        [
            [art1_lemma1, art1_lemma2, …],
            [art2_lemma1, art2_lemma2, …],
            ...
        ]
    """
    output = []
    with open(json_file, 'r') as f:
        contenu = json.load(f)

    if len(cat_gram) == 0:
        # для каждой статьи
        for article in contenu['articles']:
            tokens = []
            for token in article['analyse']:
                if lem_forme == 'lemme':
                    lemme = token['lemme']
                else:
                    lemme = token['forme']
                if lemme is not None:
                    tokens.append(lemme)
            output.append(tokens)
    else:
        for article in contenu['articles']:
            tokens = []
            for token in article['analyse']:
                if lem_forme == 'lemme':
                    lemme = token['lemme']
                else:
                    lemme = token['forme']
                # фильтруем грамматические категории
                categorie = token['pos']
                if lemme is not None and categorie in cat_gram:
                    tokens.append(lemme)
            output.append(tokens)
    return output

parser = argparse.ArgumentParser()
parser.add_argument("-f", help="format à lire (xml, json ou pkl)", default="xml")
parser.add_argument("-l", help="sélectionne le format (lemme ou forme)", default="lemme")
parser.add_argument("source_file", help="name of the file containing data to analyse")
parser.add_argument("categories", nargs="*", help="catégories grammaticales à retenir")
args = parser.parse_args()

if args.f == 'xml':
    conversion = conversion_xml
elif args.f == 'json':
    conversion = conversion_json
elif args.f == 'pkl':
    conversion = conversion_pkl
else:
    print("Méthode non disponible", file=sys.stderr)
    sys.exit()

# gestion des catégories grammaticales
cat_gram = args.categories

if args.l == 'lemme':
    docs = conversion(args.source_file, 'lemme', cat_gram)
elif args.l == 'forme':
    docs = conversion(args.source_file, 'forme', cat_gram)
else:
    print("Sélectionnez 'lemme' ou 'forme'", file=sys.stderr)
    sys.exit()

# on retire les nombres
docs = [[token for token in doc if not token.isnumeric()] for doc in docs]
# on retire les token d'une seule lettre
docs = [[token for token in doc if len(token) > 1] for doc in docs]

print(f"Nombre de lemmes : {len(docs)}")
print("MON DOCS")
print(docs)

# Compute bigrams.
from gensim.models import Phrases

# Add bigrams and trigrams to docs (only ones that appear 20 times or more).
bigram = Phrases(docs, min_count=20)
for idx in range(len(docs)):
    for token in bigram[docs[idx]]:
        if '_' in token:
            # Token is a bigram, add to document.
            docs[idx].append(token)

###############################################################################
# We remove rare words and common words based on their *document frequency*.
# Below we remove words that appear in less than 20 documents or in more than
# 50% of the documents. Consider trying to remove words only based on their
# frequency, or maybe combining that with this approach.
#

# Remove rare and common tokens.
from gensim.corpora import Dictionary

# Create a dictionary representation of the documents.
dictionary = Dictionary(docs)

# Filter out words that occur less than 20 documents, or more than 50% of the documents.
dictionary.filter_extremes(no_below=20, no_above=0.5)

###############################################################################
# Finally, we transform the documents to a vectorized form. We simply compute
# the frequency of each word, including the bigrams.
#

# Bag-of-words representation of the documents.
corpus = [dictionary.doc2bow(doc) for doc in docs]

###############################################################################
# Let's see how many tokens and documents we have to train on.
#

print('Number of unique tokens: %d' % len(dictionary))
print('Number of documents: %d' % len(corpus))

###############################################################################
# Training
# --------
#
# We are ready to train the LDA model. We will first discuss how to set some of
# the training parameters.
#
# First of all, the elephant in the room: how many topics do I need? There is
# really no easy answer for this, it will depend on both your data and your
# application. I have used 10 topics here because I wanted to have a few topics
# that I could interpret and "label", and because that turned out to give me
# reasonably good results. You might not need to interpret all your topics, so
# you could use a large number of topics, for example 100.
#
# ``chunksize`` controls how many documents are processed at a time in the
# training algorithm. Increasing chunksize will speed up training, at least as
# long as the chunk of documents easily fit into memory. I've set ``chunksize =
# 2000``, which is more than the amount of documents, so I process all the
# data in one go. Chunksize can however influence the quality of the model, as
# discussed in Hoffman and co-authors [2], but the difference was not
# substantial in this case.
#
# ``passes`` controls how often we train the model on the entire corpus.
# Another word for passes might be "epochs". ``iterations`` is somewhat
# technical, but essentially it controls how often we repeat a particular loop
# over each document. It is important to set the number of "passes" and
# "iterations" high enough.
#
# I suggest the following way to choose iterations and passes. First, enable
# logging (as described in many Gensim tutorials), and set ``eval_every = 1``
# in ``LdaModel``. When training the model look for a line in the log that
# looks something like this::
#
#    2016-06-21 15:40:06,753 - gensim.models.ldamodel - DEBUG - 68/1566 documents converged within 400 iterations
#
# If you set ``passes = 20`` you will see this line 20 times. Make sure that by
# the final passes, most of the documents have converged. So you want to choose
# both passes and iterations to be high enough for this to happen.
#
# We set ``alpha = 'auto'`` and ``eta = 'auto'``. Again this is somewhat
# technical, but essentially we are automatically learning two parameters in
# the model that we usually would have to specify explicitly.
#
###############################################################################
# VISUALISATION
###############################################################################

# Train LDA model.
from gensim.models import LdaModel
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis

# Set training parameters.
num_topics = 5
chunksize = 2000
passes = 20
iterations = 400
eval_every = None  # Don't evaluate model perplexity, takes too much time.

# Make an index to word dictionary.
temp = dictionary[0]  # This is only to "load" the dictionary.
id2word = dictionary.id2token

model = LdaModel(
    corpus=corpus,
    id2word=id2word,
    chunksize=chunksize,
    alpha='auto',
    eta='auto',
    iterations=iterations,
    num_topics=num_topics,
    passes=passes,
    eval_every=eval_every
)

top_topics = model.top_topics(corpus)

# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.
avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics
print('Average topic coherence: %.4f.' % avg_topic_coherence)

from pprint import pprint
pprint(top_topics)

# Create visualization
lda_visualization = gensimvis.prepare(model, corpus, dictionary)

# Save visualization to HTML file
pyLDAvis.save_html(lda_visualization, 'lda_visualization.html')

# Display visualization
pyLDAvis.display(lda_visualization)
						</code>
					</pre>
				</div>
				<div class="column is-one-fifth"></div>
			</div>
			<!--extract_many-->
			<h1 class="title has-text-centered" style="color: #a92927; font-size: 40px; letter-spacing: -0.02em; font-variant-caps: small-caps;">extract_many.py</h1>
			<div class="columns">
				<div class="column is-one-fifth"></div>
				<div class="column" style="border-left: 5px solid #a92927; overflow: auto; max-height: 1000px;">
					<p class="block">
						Ce programme nous permet d'analyser le corpus avec l'un des trois analyseurs disponibles (spacy, stanza, trankit) puis de sortir l'analyse dans l'un des trois formats disponibles (.xml, .pkl, .json).
					</p>
					<pre>
						<code>
import argparse
import re
import sys
from datetime import date
from pathlib import Path
from typing import Optional, List

from export_json import write_json
from tqdm import tqdm

import extract_un_fil as euf
from datastructures import Corpus
from export_pickle import write_pickle
from export_xml import write_xml

MONTHS = ["Jan",
          "Feb",
          "Mar",
          "Apr",
          "May",
          "Jun",
          "Jul",
          "Aug",
          "Sep",
          "Oct",
          "Nov",
          "Dec"]

DAYS = [f"{x:02}" for x in range(1,32)]

CAT_CODES =  {
            'une' : '0,2-3208,1-0,0',
            'international' : '0,2-3210,1-0,0',
            'europe' : '0,2-3214,1-0,0',
            'societe' :	'0,2-3224,1-0,0',
            'idees'	: '0,2-3232,1-0,0',
            'economie':	'0,2-3234,1-0,0',
            'actualite-medias':	'0,2-3236,1-0,0',
            'sport': '0,2-3242,1-0,0',
            'planete': '0,2-3244,1-0,0',
            'culture': '0,2-3246,1-0,0',
            'livres' : '0,2-3260,1-0,0',
            'cinema' : '0,2-3476,1-0,0',
            'voyage' : '0,2-3546,1-0,0',
            'technologies': '0,2-651865,1-0,0',
            'politique' : '0,57-0,64-823353,0',
            'sciences' : 'env_sciences'
            }


def categorie_of_ficname(ficname: str)-> Optional[str]:
    for nom, code in CAT_CODES.items():
        if code in ficname:
            return nom
    return None

def convert_month(m:str) -> int:
   return MONTHS.index(m) + 1

def parcours_path(corpus_dir:Path, categories: Optional[List[str]]=None, start_date: Optional[date]=None, end_date: Optional[date]=None):
    if categories is not None and len(categories) > 0:
        categories = [CAT_CODES[c] for c in categories]
    else:
        categories = CAT_CODES.values() # on prend tout

    for month_dir in corpus_dir.iterdir():
        if month_dir.name not in MONTHS:
            # on ignore les dossiers qui ne sont pas des mois
            continue
        m = convert_month(month_dir.name)
        for day_dir in month_dir.iterdir():
            if day_dir.name not in DAYS:
                # on ignore les dossiers qui ne sont pas des jours
                continue
            d = date.fromisoformat(f"2022-{m:02}-{day_dir.name}")
            if (start_date is None or start_date <= d) and (end_date is None or end_date >= d):
                for time_dir in day_dir.iterdir():
                    if re.match(r"\d\d-\d\d-\d\d", time_dir.name):
                        for fic in time_dir.iterdir():
                            if fic.name.endswith(".xml") and any([c in fic.name for c in categories]):
                                c = categorie_of_ficname(fic.name)
                                yield fic, d.isoformat(), c




if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", help="méthode de parsing (et, re ou fp)", default="et")
    parser.add_argument("-s", help="start date (iso format)", default="2022-01-01")
    parser.add_argument("-e", help="end date (iso format)", default="2023-01-01")
    parser.add_argument("-o", help="output file", required=True)
    parser.add_argument("-f", help="output format (xml by default)", default="xml")
    parser.add_argument("-a", help="analyseur", default="sp")
    parser.add_argument("corpus_dir", help="root dir of the corpus data")
    parser.add_argument("categories",nargs="*", help="catégories à retenir")
    args = parser.parse_args()
    if args.m == 'et':
        func = euf.extract_et
    elif args.m == 're':
        func = euf.extract_re
    elif args.m == 'fp':
        func = euf.extract_feedparser
    else:
        print("méthode non disponible", file=sys.stderr)
		sys.exit()
	if args.a == "sp":
		import analyse_sp as analyse
	elif args.a == "tk":
		import analyse_tk as analyse
	elif args.a == "stz":
		import analyse_stz as analyse
	else:
		print("analyseur non disponible", file=sys.stderr)
        sys.exit()
    # création du corpus
    corpus = Corpus(args.categories, args.s, args.e, args.corpus_dir, [])
    for f, d, c in parcours_path(Path(args.corpus_dir),
            start_date=date.fromisoformat(args.s),
            end_date=date.fromisoformat(args.e),
            categories=args.categories):
        # fichier par fichier
        for article in func(f, d, c):
            # parcours article par article
            corpus.articles.append(article)
    parser = analyse.create_parser()
    for a in tqdm(corpus.articles):
        analyse.analyse_article(parser, a)

    # -> corpus
    if args.f == "xml":
        write_xml(corpus, args.o)
    elif args.f == "json":
        write_json(corpus, args.o)
    elif args.f == "pickle":
        write_pickle(corpus, args.o)

    else:
        # note: ce serait mieux de tester au début de l'execution
        print("format de sortie non reconnu", file=sys.stderr)

						</code>
					</pre>
				</div>
				<div class="column is-one-fifth"></div>
			</div>
			<!--analyseurs-->
			<h1 class="title has-text-centered" style="color: #a92927; font-size: 40px; letter-spacing: -0.02em; font-variant-caps: small-caps;">Analyseurs</h1>
			<div class="columns">
				<div class="column is-one-fifth"></div>
				<div class="column" style="border-left: 5px solid #a92927; overflow: auto; max-height: 1000px;">
					<!--spacy-->
					<h2 class="title has-text-centered" style="color: #06213e; font-size: 32px; letter-spacing: -0.02em;">Spacy</h2>
					<p class="block">
						Ce programme nous permet d'anoter le corpus avec spacy.
					</p>
					<pre>
						<code>
import spacy

from collections import namedtuple
from dataclasses import dataclass

from datastructures import Token, Article


def create_parser():
    return spacy.load("fr_core_news_md")


def analyse_article(parser, article: Article) -> Article:
    result = parser( (article.titre or "" ) + "\n" + (article.description or ""))
    output = []
    for token in result:
        output.append(Token(token.text, token.lemma_, token.pos_))
    article.analyse = output
    return article
						</code>
					</pre>
					<!--stanza-->
					<h2 class="title has-text-centered" style="color: #06213e; font-size: 32px; letter-spacing: -0.02em;">Stanza</h2>
					<p class="block">
						Ce programme nous permet d'anoter le corpus avec stanza.
					</p>
					<pre>
						<code>
import stanza
from datastructures import Token, Article

def create_parser():
    stanza.download("fr")  # Télécharger les modèles de la langue française
    return stanza.Pipeline("fr", processors="tokenize,lemma,pos")  # Initialiser le pipeline pour le français

def analyse_article(parser, article: Article) -> Article:
    result = parser((article.titre or "") + "\n" + (article.description or ""))
    output = []
    for sent in result.sentences:
        for word in sent.words:
            output.append(Token(word.text, word.lemma, word.upos))
    article.analyse = output
    return article
						</code>
					</pre>
					<!--trankit-->
					<h2 class="title has-text-centered" style="color: #06213e; font-size: 32px; letter-spacing: -0.02em;">Trankit</h2>
					<p class="block">
						Ce programme nous permet d'anoter le corpus avec trankit.
					</p>
					<pre>
						<code>
import trankit

from collections import namedtuple
from dataclasses import dataclass

from datastructures import Token, Article


def create_parser():
    return trankit.Pipeline('french', gpu=False)


def analyse_article(parser, article: Article) -> Article:
    result = parser( (article.titre or "" ) + "\n" + (article.description or ""))
    output = []
    for sentence in result['sentences']:
        for token in sentence['tokens']:
            if 'expanded' not in token.keys():
                token['expanded'] = [token]
            for w in token['expanded']:
                output.append(Token(w['text'], w['lemma'], w['upos']))
    article.analyse = output
    return article
						</code>
					</pre>
				</div>
				<div class="column is-one-fifth"></div>
			</div>
			<!--export-->
			<h1 class="title has-text-centered" style="color: #a92927; font-size: 40px; letter-spacing: -0.02em; font-variant-caps: small-caps;">Exports</h1>
			<div class="columns">
				<div class="column is-one-fifth"></div>
				<div class="column" style="border-left: 5px solid #a92927; overflow: auto; max-height: 1000px;">
					<!--xml-->
					<h2 class="title has-text-centered" style="color: #06213e; font-size: 32px; letter-spacing: -0.02em;">XML</h2>
					<p class="block">
						Ce programme nous permet d'exporter au format .xml.
					</p>
					<pre>
						<code>
from typing import List
from xml.etree import ElementTree as ET

from datastructures import Corpus, Article, Token



def analyse_to_xml(tokens: List[Token]) -> ET.Element:
    root = ET.Element("analyse")
    for tok in tokens:
        tok_element = ET.SubElement(root, "token")
        tok_element.attrib['forme'] = tok.forme 
        tok_element.attrib['pos'] = tok.pos 
        tok_element.attrib['lemme'] = tok.lemme 
    return root


def article_to_xml(article: Article) -> ET.Element:
    art = ET.Element("article")
    art.attrib['date'] = article.date
    art.attrib['categorie'] = article.categorie
    title = ET.SubElement(art, "title")
    description = ET.SubElement(art, "description")
    title.text = article.titre
    description.text = article.description
    art.append(analyse_to_xml(article.analyse))
    return art

def write_xml(corpus: Corpus, destination: str):
    root = ET.Element("corpus")
    root.attrib['begin'] = corpus.begin
    root.attrib['end'] = corpus.end
    categories = ET.SubElement(root, "categories")
    for c in corpus.categories:
        ET.SubElement(categories, "cat").text = c
    content = ET.SubElement(root, "content")
    for article in corpus.articles:
        art_xml = article_to_xml(article)
        content.append(art_xml)
    tree = ET.ElementTree(root)
    ET.indent(tree)
    tree.write(destination)
						</code>
					</pre>
					<!--pickle-->
					<h2 class="title has-text-centered" style="color: #06213e; font-size: 32px; letter-spacing: -0.02em;">Pickle</h2>
					<p class="block">
						Ce programme nous permet d'exporter au format .pkl.
					</p>
					<pre>
						<code>
import pickle
from datastructures import Corpus, Article

# extension pickle : .pkl

def write_pickle(corpus:Corpus, destination:str):
    with open(destination, "wb") as fout:
        pickle.dump(corpus, fout)
						</code>
					</pre>
					<!--json-->
					<h2 class="title has-text-centered" style="color: #06213e; font-size: 32px; letter-spacing: -0.02em;">Json</h2>
					<p class="block">
						Ce programme nous permet d'exporter au format .json.
					</p>
					<pre>
						<code>
import json
from dataclasses import asdict

from datastructures import Corpus, Article


def write_json(corpus:Corpus, destination:str):
    with open(destination, "w") as fout:
        json.dump(asdict(corpus), fout)
						</code>
					</pre>
				</div>
				<div class="column is-one-fifth"></div>
			</div>
    	</div>
	</body>
</html>